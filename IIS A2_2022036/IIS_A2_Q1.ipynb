{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjl7tiVo5yz1"
      },
      "outputs": [],
      "source": [
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk .corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer"
      ],
      "metadata": {
        "id": "pqriZ8XC6SSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('wordnet')\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoUAp_rz6VtT",
        "outputId": "865635dc-0da1-4856-ea5e-4ce4a74ddcb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"Artificial Intelligence (AI) is a rapidly growing field that has the potential to revolutionize many aspects of our lives. AI refers to the development of intelligent machines that can perform tasks that typically require human intelligence, such as recognizing speech, making decisions, and solving problems. One of the key features of AI is machine learning, which involves training machines to learn from data and improve their performance over time. Machine learning algorithms can analyze large amounts of data to identify patterns and make predictions, and they can adapt their behavior based on feedback. AI is already being used in a variety of applications, from speech recognition and natural language processing to image recognition and autonomous vehicles. It is also being used to improve healthcare, finance, and education, among other fields. However, AI also poses significant challenges and concerns. One of the biggest concerns is the potential for AI to replace human workers, which could lead to widespread job loss. There are also concerns about the potential misuse of AI, particularly in the development of autonomous weapons and other systems that could be used to harm people. To address these challenges and concerns, it is important for researchers and policymakers to work together to ensure that AI is developed in a responsible and ethical manner. This includes ensuring that AI is designed with human values and ethical considerations in mind, and that it is transparent and accountable. In summary, AI has enormous potential to benefit society in a variety of ways, but it also presents significant challenges and concerns. By working together to develop AI in a responsible and ethical manner, we can help ensure that it is used to improve people's lives and advance human progress.\"\n"
      ],
      "metadata": {
        "id": "eexhxl266YW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#(i)Tokenizes the text into individual words or phrases.\n",
        "tokens=nltk.word_tokenize(text)\n",
        "print(\"(i).........................................................................................\")\n",
        "print(\"Tokenized text: \", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acN7hHVM6aRf",
        "outputId": "90d9fec8-3a3e-4e97-c8be-ea9f7a8af782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(i).........................................................................................\n",
            "Tokenized text:  ['Artificial', 'Intelligence', '(', 'AI', ')', 'is', 'a', 'rapidly', 'growing', 'field', 'that', 'has', 'the', 'potential', 'to', 'revolutionize', 'many', 'aspects', 'of', 'our', 'lives', '.', 'AI', 'refers', 'to', 'the', 'development', 'of', 'intelligent', 'machines', 'that', 'can', 'perform', 'tasks', 'that', 'typically', 'require', 'human', 'intelligence', ',', 'such', 'as', 'recognizing', 'speech', ',', 'making', 'decisions', ',', 'and', 'solving', 'problems', '.', 'One', 'of', 'the', 'key', 'features', 'of', 'AI', 'is', 'machine', 'learning', ',', 'which', 'involves', 'training', 'machines', 'to', 'learn', 'from', 'data', 'and', 'improve', 'their', 'performance', 'over', 'time', '.', 'Machine', 'learning', 'algorithms', 'can', 'analyze', 'large', 'amounts', 'of', 'data', 'to', 'identify', 'patterns', 'and', 'make', 'predictions', ',', 'and', 'they', 'can', 'adapt', 'their', 'behavior', 'based', 'on', 'feedback', '.', 'AI', 'is', 'already', 'being', 'used', 'in', 'a', 'variety', 'of', 'applications', ',', 'from', 'speech', 'recognition', 'and', 'natural', 'language', 'processing', 'to', 'image', 'recognition', 'and', 'autonomous', 'vehicles', '.', 'It', 'is', 'also', 'being', 'used', 'to', 'improve', 'healthcare', ',', 'finance', ',', 'and', 'education', ',', 'among', 'other', 'fields', '.', 'However', ',', 'AI', 'also', 'poses', 'significant', 'challenges', 'and', 'concerns', '.', 'One', 'of', 'the', 'biggest', 'concerns', 'is', 'the', 'potential', 'for', 'AI', 'to', 'replace', 'human', 'workers', ',', 'which', 'could', 'lead', 'to', 'widespread', 'job', 'loss', '.', 'There', 'are', 'also', 'concerns', 'about', 'the', 'potential', 'misuse', 'of', 'AI', ',', 'particularly', 'in', 'the', 'development', 'of', 'autonomous', 'weapons', 'and', 'other', 'systems', 'that', 'could', 'be', 'used', 'to', 'harm', 'people', '.', 'To', 'address', 'these', 'challenges', 'and', 'concerns', ',', 'it', 'is', 'important', 'for', 'researchers', 'and', 'policymakers', 'to', 'work', 'together', 'to', 'ensure', 'that', 'AI', 'is', 'developed', 'in', 'a', 'responsible', 'and', 'ethical', 'manner', '.', 'This', 'includes', 'ensuring', 'that', 'AI', 'is', 'designed', 'with', 'human', 'values', 'and', 'ethical', 'considerations', 'in', 'mind', ',', 'and', 'that', 'it', 'is', 'transparent', 'and', 'accountable', '.', 'In', 'summary', ',', 'AI', 'has', 'enormous', 'potential', 'to', 'benefit', 'society', 'in', 'a', 'variety', 'of', 'ways', ',', 'but', 'it', 'also', 'presents', 'significant', 'challenges', 'and', 'concerns', '.', 'By', 'working', 'together', 'to', 'develop', 'AI', 'in', 'a', 'responsible', 'and', 'ethical', 'manner', ',', 'we', 'can', 'help', 'ensure', 'that', 'it', 'is', 'used', 'to', 'improve', 'people', \"'s\", 'lives', 'and', 'advance', 'human', 'progress', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#(ii)Removes stop words (e.g., ”the”, ”and”, ”a”, etc.) from the tokenized text.\n",
        "stop_words=set(stopwords.words('english'))\n",
        "new_text = []\n",
        "for token in tokens:\n",
        "    if not token.lower() in stop_words:\n",
        "        new_text.append(token)\n",
        "print(\"(ii).........................................................................................\")\n",
        "print(\"Filtered text: \", new_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JyOeEIA6aeJ",
        "outputId": "9a3637f1-d07d-4f63-8923-e559fdfb13d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(ii).........................................................................................\n",
            "Filtered text:  ['Artificial', 'Intelligence', '(', 'AI', ')', 'rapidly', 'growing', 'field', 'potential', 'revolutionize', 'many', 'aspects', 'lives', '.', 'AI', 'refers', 'development', 'intelligent', 'machines', 'perform', 'tasks', 'typically', 'require', 'human', 'intelligence', ',', 'recognizing', 'speech', ',', 'making', 'decisions', ',', 'solving', 'problems', '.', 'One', 'key', 'features', 'AI', 'machine', 'learning', ',', 'involves', 'training', 'machines', 'learn', 'data', 'improve', 'performance', 'time', '.', 'Machine', 'learning', 'algorithms', 'analyze', 'large', 'amounts', 'data', 'identify', 'patterns', 'make', 'predictions', ',', 'adapt', 'behavior', 'based', 'feedback', '.', 'AI', 'already', 'used', 'variety', 'applications', ',', 'speech', 'recognition', 'natural', 'language', 'processing', 'image', 'recognition', 'autonomous', 'vehicles', '.', 'also', 'used', 'improve', 'healthcare', ',', 'finance', ',', 'education', ',', 'among', 'fields', '.', 'However', ',', 'AI', 'also', 'poses', 'significant', 'challenges', 'concerns', '.', 'One', 'biggest', 'concerns', 'potential', 'AI', 'replace', 'human', 'workers', ',', 'could', 'lead', 'widespread', 'job', 'loss', '.', 'also', 'concerns', 'potential', 'misuse', 'AI', ',', 'particularly', 'development', 'autonomous', 'weapons', 'systems', 'could', 'used', 'harm', 'people', '.', 'address', 'challenges', 'concerns', ',', 'important', 'researchers', 'policymakers', 'work', 'together', 'ensure', 'AI', 'developed', 'responsible', 'ethical', 'manner', '.', 'includes', 'ensuring', 'AI', 'designed', 'human', 'values', 'ethical', 'considerations', 'mind', ',', 'transparent', 'accountable', '.', 'summary', ',', 'AI', 'enormous', 'potential', 'benefit', 'society', 'variety', 'ways', ',', 'also', 'presents', 'significant', 'challenges', 'concerns', '.', 'working', 'together', 'develop', 'AI', 'responsible', 'ethical', 'manner', ',', 'help', 'ensure', 'used', 'improve', 'people', \"'s\", 'lives', 'advance', 'human', 'progress', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#(iii)Performs stemming or lemmatization to reduce words to their baseform.\n",
        "stem = []\n",
        "for word in new_text:\n",
        "    stemmed_word = stemmer.stem(word)\n",
        "    stem.append(stemmed_word)\n",
        "lemm = []\n",
        "for word in new_text:\n",
        "    lemmatized_word = lemmatizer.lemmatize(word)\n",
        "    lemm.append(lemmatized_word)\n",
        "print(\"(iii).........................................................................................\")\n",
        "print(\"Stemmed tokens:\", stem)\n",
        "print(\"Lemmatized tokens:\", lemm)    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcpjN3PG6amZ",
        "outputId": "5387db9e-ef5b-46de-f740-eb41e0283b44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(iii).........................................................................................\n",
            "Stemmed tokens: ['artifici', 'intellig', '(', 'ai', ')', 'rapidli', 'grow', 'field', 'potenti', 'revolution', 'mani', 'aspect', 'live', '.', 'ai', 'refer', 'develop', 'intellig', 'machin', 'perform', 'task', 'typic', 'requir', 'human', 'intellig', ',', 'recogn', 'speech', ',', 'make', 'decis', ',', 'solv', 'problem', '.', 'one', 'key', 'featur', 'ai', 'machin', 'learn', ',', 'involv', 'train', 'machin', 'learn', 'data', 'improv', 'perform', 'time', '.', 'machin', 'learn', 'algorithm', 'analyz', 'larg', 'amount', 'data', 'identifi', 'pattern', 'make', 'predict', ',', 'adapt', 'behavior', 'base', 'feedback', '.', 'ai', 'alreadi', 'use', 'varieti', 'applic', ',', 'speech', 'recognit', 'natur', 'languag', 'process', 'imag', 'recognit', 'autonom', 'vehicl', '.', 'also', 'use', 'improv', 'healthcar', ',', 'financ', ',', 'educ', ',', 'among', 'field', '.', 'howev', ',', 'ai', 'also', 'pose', 'signific', 'challeng', 'concern', '.', 'one', 'biggest', 'concern', 'potenti', 'ai', 'replac', 'human', 'worker', ',', 'could', 'lead', 'widespread', 'job', 'loss', '.', 'also', 'concern', 'potenti', 'misus', 'ai', ',', 'particularli', 'develop', 'autonom', 'weapon', 'system', 'could', 'use', 'harm', 'peopl', '.', 'address', 'challeng', 'concern', ',', 'import', 'research', 'policymak', 'work', 'togeth', 'ensur', 'ai', 'develop', 'respons', 'ethic', 'manner', '.', 'includ', 'ensur', 'ai', 'design', 'human', 'valu', 'ethic', 'consider', 'mind', ',', 'transpar', 'account', '.', 'summari', ',', 'ai', 'enorm', 'potenti', 'benefit', 'societi', 'varieti', 'way', ',', 'also', 'present', 'signific', 'challeng', 'concern', '.', 'work', 'togeth', 'develop', 'ai', 'respons', 'ethic', 'manner', ',', 'help', 'ensur', 'use', 'improv', 'peopl', \"'s\", 'live', 'advanc', 'human', 'progress', '.']\n",
            "Lemmatized tokens: ['Artificial', 'Intelligence', '(', 'AI', ')', 'rapidly', 'growing', 'field', 'potential', 'revolutionize', 'many', 'aspect', 'life', '.', 'AI', 'refers', 'development', 'intelligent', 'machine', 'perform', 'task', 'typically', 'require', 'human', 'intelligence', ',', 'recognizing', 'speech', ',', 'making', 'decision', ',', 'solving', 'problem', '.', 'One', 'key', 'feature', 'AI', 'machine', 'learning', ',', 'involves', 'training', 'machine', 'learn', 'data', 'improve', 'performance', 'time', '.', 'Machine', 'learning', 'algorithm', 'analyze', 'large', 'amount', 'data', 'identify', 'pattern', 'make', 'prediction', ',', 'adapt', 'behavior', 'based', 'feedback', '.', 'AI', 'already', 'used', 'variety', 'application', ',', 'speech', 'recognition', 'natural', 'language', 'processing', 'image', 'recognition', 'autonomous', 'vehicle', '.', 'also', 'used', 'improve', 'healthcare', ',', 'finance', ',', 'education', ',', 'among', 'field', '.', 'However', ',', 'AI', 'also', 'pose', 'significant', 'challenge', 'concern', '.', 'One', 'biggest', 'concern', 'potential', 'AI', 'replace', 'human', 'worker', ',', 'could', 'lead', 'widespread', 'job', 'loss', '.', 'also', 'concern', 'potential', 'misuse', 'AI', ',', 'particularly', 'development', 'autonomous', 'weapon', 'system', 'could', 'used', 'harm', 'people', '.', 'address', 'challenge', 'concern', ',', 'important', 'researcher', 'policymakers', 'work', 'together', 'ensure', 'AI', 'developed', 'responsible', 'ethical', 'manner', '.', 'includes', 'ensuring', 'AI', 'designed', 'human', 'value', 'ethical', 'consideration', 'mind', ',', 'transparent', 'accountable', '.', 'summary', ',', 'AI', 'enormous', 'potential', 'benefit', 'society', 'variety', 'way', ',', 'also', 'present', 'significant', 'challenge', 'concern', '.', 'working', 'together', 'develop', 'AI', 'responsible', 'ethical', 'manner', ',', 'help', 'ensure', 'used', 'improve', 'people', \"'s\", 'life', 'advance', 'human', 'progress', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#(iv)Counts the frequency of each word or phrase in the text.\n",
        "stemmed_freq = {}\n",
        "lemmatized_freq = {}\n",
        "\n",
        "for word in stem:\n",
        "    if word in stemmed_freq:\n",
        "        stemmed_freq[word] += 1\n",
        "    else:\n",
        "        stemmed_freq[word] = 1\n",
        "\n",
        "for word in lemm:\n",
        "    if word in lemmatized_freq:\n",
        "        lemmatized_freq[word] += 1\n",
        "    else:\n",
        "        lemmatized_freq[word] = 1\n",
        "print(\"(iv).........................................................................................\")\n",
        "print(\"Stemmed token frequency:\", stemmed_freq)\n",
        "print(\"Lemmatized token frequency:\", lemmatized_freq)        "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb_ETzcy6at6",
        "outputId": "fda2ff84-b5ba-4b2e-d00c-93a04132550f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(iv).........................................................................................\n",
            "Stemmed token frequency: {'artifici': 1, 'intellig': 3, '(': 1, 'ai': 11, ')': 1, 'rapidli': 1, 'grow': 1, 'field': 2, 'potenti': 4, 'revolution': 1, 'mani': 1, 'aspect': 1, 'live': 2, '.': 13, 'refer': 1, 'develop': 4, 'machin': 4, 'perform': 2, 'task': 1, 'typic': 1, 'requir': 1, 'human': 4, ',': 17, 'recogn': 1, 'speech': 2, 'make': 2, 'decis': 1, 'solv': 1, 'problem': 1, 'one': 2, 'key': 1, 'featur': 1, 'learn': 3, 'involv': 1, 'train': 1, 'data': 2, 'improv': 3, 'time': 1, 'algorithm': 1, 'analyz': 1, 'larg': 1, 'amount': 1, 'identifi': 1, 'pattern': 1, 'predict': 1, 'adapt': 1, 'behavior': 1, 'base': 1, 'feedback': 1, 'alreadi': 1, 'use': 4, 'varieti': 2, 'applic': 1, 'recognit': 2, 'natur': 1, 'languag': 1, 'process': 1, 'imag': 1, 'autonom': 2, 'vehicl': 1, 'also': 4, 'healthcar': 1, 'financ': 1, 'educ': 1, 'among': 1, 'howev': 1, 'pose': 1, 'signific': 2, 'challeng': 3, 'concern': 5, 'biggest': 1, 'replac': 1, 'worker': 1, 'could': 2, 'lead': 1, 'widespread': 1, 'job': 1, 'loss': 1, 'misus': 1, 'particularli': 1, 'weapon': 1, 'system': 1, 'harm': 1, 'peopl': 2, 'address': 1, 'import': 1, 'research': 1, 'policymak': 1, 'work': 2, 'togeth': 2, 'ensur': 3, 'respons': 2, 'ethic': 3, 'manner': 2, 'includ': 1, 'design': 1, 'valu': 1, 'consider': 1, 'mind': 1, 'transpar': 1, 'account': 1, 'summari': 1, 'enorm': 1, 'benefit': 1, 'societi': 1, 'way': 1, 'present': 1, 'help': 1, \"'s\": 1, 'advanc': 1, 'progress': 1}\n",
            "Lemmatized token frequency: {'Artificial': 1, 'Intelligence': 1, '(': 1, 'AI': 11, ')': 1, 'rapidly': 1, 'growing': 1, 'field': 2, 'potential': 4, 'revolutionize': 1, 'many': 1, 'aspect': 1, 'life': 2, '.': 13, 'refers': 1, 'development': 2, 'intelligent': 1, 'machine': 3, 'perform': 1, 'task': 1, 'typically': 1, 'require': 1, 'human': 4, 'intelligence': 1, ',': 17, 'recognizing': 1, 'speech': 2, 'making': 1, 'decision': 1, 'solving': 1, 'problem': 1, 'One': 2, 'key': 1, 'feature': 1, 'learning': 2, 'involves': 1, 'training': 1, 'learn': 1, 'data': 2, 'improve': 3, 'performance': 1, 'time': 1, 'Machine': 1, 'algorithm': 1, 'analyze': 1, 'large': 1, 'amount': 1, 'identify': 1, 'pattern': 1, 'make': 1, 'prediction': 1, 'adapt': 1, 'behavior': 1, 'based': 1, 'feedback': 1, 'already': 1, 'used': 4, 'variety': 2, 'application': 1, 'recognition': 2, 'natural': 1, 'language': 1, 'processing': 1, 'image': 1, 'autonomous': 2, 'vehicle': 1, 'also': 4, 'healthcare': 1, 'finance': 1, 'education': 1, 'among': 1, 'However': 1, 'pose': 1, 'significant': 2, 'challenge': 3, 'concern': 5, 'biggest': 1, 'replace': 1, 'worker': 1, 'could': 2, 'lead': 1, 'widespread': 1, 'job': 1, 'loss': 1, 'misuse': 1, 'particularly': 1, 'weapon': 1, 'system': 1, 'harm': 1, 'people': 2, 'address': 1, 'important': 1, 'researcher': 1, 'policymakers': 1, 'work': 1, 'together': 2, 'ensure': 2, 'developed': 1, 'responsible': 2, 'ethical': 3, 'manner': 2, 'includes': 1, 'ensuring': 1, 'designed': 1, 'value': 1, 'consideration': 1, 'mind': 1, 'transparent': 1, 'accountable': 1, 'summary': 1, 'enormous': 1, 'benefit': 1, 'society': 1, 'way': 1, 'present': 1, 'working': 1, 'develop': 1, 'help': 1, \"'s\": 1, 'advance': 1, 'progress': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#(v)Outputs the most frequent words or phrases in the text, along with their frequency count.\n",
        "n = 10\n",
        "most_common_stemmed = sorted(stemmed_freq.items(), key=lambda x: x[1], reverse=True)[:n]\n",
        "most_common_lemmatized = sorted(lemmatized_freq.items(), key=lambda x: x[1], reverse=True)[:n]\n",
        "print(\"(v).........................................................................................\")\n",
        "print(\"Most frequent stemmed tokens (top {}):\".format(n))\n",
        "for token, freq in most_common_stemmed:\n",
        "    print(\"{}: {}\".format(token, freq))\n",
        "print(\"Most frequent lemmatized tokens (top {}):\".format(n))\n",
        "for token, freq in most_common_lemmatized:\n",
        "    print(\"{}: {}\".format(token, freq))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ivk4dM66eLv",
        "outputId": "4c2ce4e0-2bfa-4e48-ff78-1e233585cbe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(v).........................................................................................\n",
            "Most frequent stemmed tokens (top 10):\n",
            ",: 17\n",
            ".: 13\n",
            "ai: 11\n",
            "concern: 5\n",
            "potenti: 4\n",
            "develop: 4\n",
            "machin: 4\n",
            "human: 4\n",
            "use: 4\n",
            "also: 4\n",
            "Most frequent lemmatized tokens (top 10):\n",
            ",: 17\n",
            ".: 13\n",
            "AI: 11\n",
            "concern: 5\n",
            "potential: 4\n",
            "human: 4\n",
            "used: 4\n",
            "also: 4\n",
            "machine: 3\n",
            "improve: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tFTygEOt6eaW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}